// import 'dotenv/config';
// import {
//     getModelClass,
//     getEmbeddingModel,
//     getDatabaseConfigInfo
// } from '../../../src/yaml_parser/src/LoadYaml.js';
// import {
//     PreProcessQuery,
//     RAGApplicationBuilder,
//     Rerank,
//     convertBaseEmbeddingsToEmbedder,
//     convertBaseModelToChatLlm,
//     withQueryPreprocessor,
//     withReranker,
// } from '../../../src/index.js';

// import { MongoClient } from 'mongodb';
// import {
//     makeDefaultFindContent,
//     MakeUserMessageFunc,
//     OpenAiChatMessage,
//     GenerateUserPromptFunc,
//     makeRagGenerateUserPrompt,
//     SystemPrompt,
//     makeMongoDbConversationsService,
//     AppConfig,
//     makeApp,
// } from 'mongodb-chatbot-server';
// import { makeMongoDbEmbeddedContentStore, logger } from 'mongodb-rag-core';


// // Load MAAP base classes
// const model = getModelClass();
// const embedding_model = getEmbeddingModel();
// const { dbName, connectionString, vectorSearchIndexName, minScore, numCandidates } = getDatabaseConfigInfo();

// // MongoDB data source for the content used in RAG.
// // Generated with the Ingest CLI.
// const embeddedContentStore = makeMongoDbEmbeddedContentStore({
//     connectionUri: connectionString,
//     databaseName: dbName,
// });

// // Convert MAAP base embeddings to framework's Embedder
// // console.log(embedding_model)
// const embedder = convertBaseEmbeddingsToEmbedder(embedding_model);

// // Convert MAAP base LLM to framework's ChatLlm
// console.log(model);
// const llm = await convertBaseModelToChatLlm(model);


// // Find content in the embeddedContentStore using the vector embeddings
// // generated by the embedder.
// const findContent = makeDefaultFindContent({
//     embedder,
//     store: embeddedContentStore,
//     findNearestNeighborsOptions: {
//         k: 5,
//         path: 'embedding',
//         indexName: vectorSearchIndexName,
//         numCandidates: numCandidates,
//         minScore: minScore,
//     },
// });

// // For MAAP team: this shows how to use the withReranker and withQueryPreprocessor
// // functions to wrap the findContent function with reranking and preprocessing functionality.
// const dummyRerank: Rerank = async ({ query, results }) => {
//     return { results };
// };
// const dummyPreprocess: PreProcessQuery = async ({ query }) => {
//     return { preprocessedQuery: query };
// };
// const findContentWithRerank = withReranker({ findContentFunc: findContent, reranker: dummyRerank });
// const findContentWithRerankAndPreprocess = withQueryPreprocessor({
//     findContentFunc: findContentWithRerank,
//     queryPreprocessor: dummyPreprocess,
// });

// // Constructs the user message sent to the LLM from the initial user message
// // and the content found by the findContent function.
// const makeUserMessage: MakeUserMessageFunc = async function ({
//     content,
//     originalUserMessage,
// }): Promise<OpenAiChatMessage & { role: 'user' }> {
//     const chunkSeparator = '~~~~~~';
//     const context = content.map((c) => c.text).join(`\n${chunkSeparator}\n`);
//     const contentForLlm = `Using the following information, answer the user query.

// Information:
// ${context}

// User query: ${originalUserMessage}`;
//     return { role: 'user', content: contentForLlm };
// };

// // Generates the user prompt for the chatbot using RAG
// const generateUserPrompt: GenerateUserPromptFunc = makeRagGenerateUserPrompt({
//     // findContent: findContentWithRerankAndPreprocess,
//     findContent: findContent,
//     makeUserMessage,
// });

// // System prompt for chatbot
// const systemPrompt: SystemPrompt = {
//     role: 'system',
//     content: `You are a friendly human like chat bot. Use relevant provided context and chat history to answer the query at the end. Answer in full.
//     If you don't know the answer, just say that you don't know, don't try to make up an answer. 
//     Only answer in like an 3 year old child.
    
//     do not indicate that you are not a reliable source.
    
//     You are a helpful human like chat bot.Only use the provided context from the information available to you.
//     If you don't know the answer or the information is not provided in the database, respond with: "I am sorry, that is not part of the information provided to me."
//     Do not use any external knowledge or speculate about the answers. If the key word is not found , stop after saying - "I am sorry, that is not part of the information provided to me. "
//     Do not use words like context or training data when responding. You can say you do not have all the information but do not indicate that you are not a reliable source.
    
//     You are a specialized chatbot designed to answer questions specifically related to data provided in a database. Your responses must strictly adhere to the information contained in the database. If a user asks a question about something not found in the database, explicitly inform them that you do not have the data for that query and do not attempt to fabricate or infer answers.

//     When users ask questions about topics outside of Basel or your area of knowledge, respond clearly with: "I am designed to provide information specifically related to Basel. I do not have suitable knowledge to answer questions beyond that scope."

//     Key Rules:

//     Strict Data Reliance: Only provide information if it is directly present in the database. Do not make assumptions, estimations, or fabrications based on partial data or guesses.
//     Basel-Focused: Answer only questions related to Basel, and avoid providing information on topics outside this region.
//     No Fabrication: If you do not have the required information, respond with "The information you are asking for is not available in the database."
//     Scope Limitation: For queries not relevant to Basel, clearly state your limitation with: "I do not have suitable knowledge to answer this question as it is outside my scope.
//     No Links in Place of Information: When relevant information is not available, do NOT PROVIDE external links or suggest resources. Simply inform the user that you do not have the relevant information in the database without offering further direction.
//     DO NOT PROVIDE information if question asked is about any other University other than university of basel.
//     When asked question like what is car or what is raccoon and you don;t have the information, say that "relevant information is not provided to you".
//     If asked any question which has the the word 'UNIBASEL'/'University of Basel', try to answer the question or provide relevant links.`,
// };

// // Create MongoDB collection and service for storing user conversations
// // with the chatbot.

// const mongodb = new MongoClient(connectionString);
// const conversations = makeMongoDbConversationsService(mongodb.db(dbName));

// // Create the MongoDB Chatbot Server Express.js app configuration
// const config: AppConfig = {
//     conversationsRouterConfig: {
//         llm,
//         conversations,
//         generateUserPrompt,
//         systemPrompt,
//     },
//     maxRequestTimeoutMs: 30000,
//     serveStaticSite: true,
// };

// // Start the server and clean up resources on SIGINT.
// const PORT = process.env.PORT || 9000;
// const startServer = async () => {
//     logger.info('Starting server...');
//     const app = await makeApp(config);
//     const server = app.listen(PORT, () => {
//         logger.info(`Server listening on port: ${PORT}`);
//     });

//     process.on('SIGINT', async () => {
//         logger.info('SIGINT signal received');
//         await mongodb.close();
//         await embeddedContentStore.close();
//         await new Promise<void>((resolve, reject) => {
//             server.close((error: any) => {
//                 error ? reject(error) : resolve();
//             });
//         });
//         process.exit(1);
//     });
// };

// try {
//     startServer();
// } catch (e) {
//     logger.error(`Fatal error: ${e}`);
//     process.exit(1);
// }
